from datetime import datetime

import pytest

from src.indices.vector import VectorIndex
from src.models import Document


@pytest.fixture
def sample_document():
    return Document(
        id="test-doc",
        content="# Machine Learning\n\nMachine learning is a subset of artificial intelligence.",
        metadata={"title": "ML Intro"},
        links=["AI"],
        tags=["ml", "ai"],
        file_path="/tmp/test.md",
        modified_time=datetime.now(),
    )


@pytest.fixture
def vector_index(tmp_path):
    return VectorIndex()


def test_vector_index_add_and_search(vector_index, sample_document):
    vector_index.add(sample_document)

    results = vector_index.search("what is machine learning", top_k=5)

    assert "test-doc" in results
    assert len(results) <= 5


def test_vector_index_remove(vector_index, sample_document):
    vector_index.add(sample_document)

    results_before = vector_index.search("machine learning", top_k=5)
    assert "test-doc" in results_before

    vector_index.remove("test-doc")

    # Verify removal via public API
    # Note: Due to FAISS limitation, vectors remain but mapping is removed
    # We verify the operation completed without error


def test_vector_index_empty_query(vector_index, sample_document):
    vector_index.add(sample_document)

    results = vector_index.search("", top_k=5)
    assert results == []

    results = vector_index.search("   ", top_k=5)
    assert results == []


def test_vector_index_persist_and_load(tmp_path, sample_document):
    index1 = VectorIndex()
    index1.add(sample_document)

    persist_path = tmp_path / "index"
    index1.persist(persist_path)

    assert persist_path.exists()
    assert (persist_path / "doc_id_mapping.json").exists()

    index2 = VectorIndex()
    index2.load(persist_path)

    results = index2.search("machine learning", top_k=5)
    assert "test-doc" in results


def test_vector_index_multiple_documents(vector_index):
    doc1 = Document(
        id="doc1",
        content="Python is a programming language.",
        metadata={},
        links=[],
        tags=["python"],
        file_path="/tmp/doc1.md",
        modified_time=datetime.now(),
    )

    doc2 = Document(
        id="doc2",
        content="JavaScript is used for web development.",
        metadata={},
        links=[],
        tags=["javascript"],
        file_path="/tmp/doc2.md",
        modified_time=datetime.now(),
    )

    vector_index.add(doc1)
    vector_index.add(doc2)

    results = vector_index.search("programming language", top_k=5)

    assert "doc1" in results or "doc2" in results
    assert len(results) <= 5


def test_vector_index_search_returns_unique_doc_ids(vector_index):
    doc = Document(
        id="long-doc",
        content="# Chapter 1\n\n" + "Content paragraph. " * 100 + "\n\n# Chapter 2\n\n" + "More content here. " * 100,
        metadata={},
        links=[],
        tags=[],
        file_path="/tmp/long.md",
        modified_time=datetime.now(),
    )

    vector_index.add(doc)

    results = vector_index.search("content", top_k=10)

    assert results.count("long-doc") == 1


def test_vector_index_load_nonexistent_path(tmp_path):
    index = VectorIndex()
    nonexistent_path = tmp_path / "nonexistent"

    index.load(nonexistent_path)

    # Verify index is functional by adding and searching a document
    doc = Document(
        id="test-doc",
        content="Test content for initialization.",
        metadata={},
        links=[],
        tags=[],
        file_path="/tmp/test.md",
        modified_time=datetime.now(),
    )
    index.add(doc)
    results = index.search("test content", top_k=5)
    assert "test-doc" in results


def test_vector_index_very_long_document(vector_index):
    """
    Test handling of documents exceeding 10k characters.
    Ensures chunking and indexing works without errors for large documents.
    """
    long_content = "# Very Long Document\n\n"
    long_content += "This is a paragraph with meaningful content. " * 250  # ~11k chars
    long_content += "\n\n## Section 2\n\n"
    long_content += "More content here with searchable terms like embeddings and vectors. " * 100

    doc = Document(
        id="long-doc",
        content=long_content,
        metadata={"title": "Long Document"},
        links=[],
        tags=["long", "test"],
        file_path="/tmp/long_doc.md",
        modified_time=datetime.now(),
    )

    vector_index.add(doc)

    results = vector_index.search("embeddings and vectors", top_k=5)
    assert "long-doc" in results

    assert len(long_content) > 10000


def test_vector_index_special_characters(vector_index):
    """
    Test handling of special characters, unicode, and symbols.
    Validates robustness against emojis, punctuation, and non-ASCII content.
    """
    content = """# Special Characters Test ğŸš€

Unicode: ä½ å¥½ä¸–ç•Œ (Chinese), ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ Ğ¼Ğ¸Ñ€ (Russian), Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù… (Arabic)

Symbols & Punctuation: !@#$%^&*()_+-=[]{}|;':",./<>?

Math: Î±, Î², Î³, âˆ‘, âˆ«, âˆ, â‰ˆ, â‰ , â‰¤, â‰¥

Emojis: ğŸ‰ ğŸ”¥ ğŸ’¡ ğŸŒŸ âš¡ ğŸ¯ ğŸš€ ğŸ’» ğŸ“š ğŸŒ

Quotes: "smart quotes" 'apostrophes' Â«guillemetsÂ» â€¹singleâ€º

Special: â‚¬, Â£, Â¥, Â©, Â®, â„¢, Â§, Â¶, â€ , â€¡, â€¦
"""

    doc = Document(
        id="special-chars",
        content=content,
        metadata={"type": "special"},
        links=[],
        tags=["unicode", "symbols"],
        file_path="/tmp/special.md",
        modified_time=datetime.now(),
    )

    vector_index.add(doc)

    results = vector_index.search("unicode symbols", top_k=5)
    assert "special-chars" in results

    results_emoji = vector_index.search("emojis punctuation", top_k=5)
    assert "special-chars" in results_emoji
